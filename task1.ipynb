{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1) Run K-means clustering with Euclidean, Cosine and Jarcard similarity. Specify K= the\n",
    "number of categorical values of y (the number of classifications). Compare the SSEs of\n",
    "Euclidean-K-means, Cosine-K-means, Jarcard-K-means. Which method is better?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def euclidean_distance(point1, point2):\n",
    "    return np.sqrt(np.sum((point1 - point2)**2))\n",
    "\n",
    "def cosine_similarity(point1, point2):\n",
    "    dot_product = np.dot(point1, point2)\n",
    "    norm_product = np.linalg.norm(point1) * np.linalg.norm(point2)\n",
    "    return 1 - (dot_product / norm_product)\n",
    "\n",
    "def jaccard_similarity(point1, point2):\n",
    "    intersection = len(np.intersect1d(point1, point2))\n",
    "    union = len(np.union1d(point1, point2))\n",
    "    return 1 - (intersection / union)\n",
    "\n",
    "def k_means(data, k, distance_function, max_iterations=100, tolerance=1e-4, max_iterations_without_change=10):\n",
    "    centroids = data[np.random.choice(range(len(data)), k, replace=False)]\n",
    "    \n",
    "    for _ in range(max_iterations):\n",
    "        clusters = [[] for _ in range(k)]\n",
    "        for point in data:\n",
    "            distances = [distance_function(point, centroid) for centroid in centroids]\n",
    "            cluster_assignment = np.argmin(distances)\n",
    "            clusters[cluster_assignment].append(point)\n",
    "        \n",
    "        new_centroids = [np.mean(cluster, axis=0) for cluster in clusters]\n",
    "        \n",
    "        if np.all(np.abs(np.array(new_centroids) - np.array(centroids)) < tolerance):\n",
    "            break\n",
    "        \n",
    "        centroids = new_centroids\n",
    "    \n",
    "    sse = sum(np.sum((np.array(cluster) - centroid)**2) for cluster, centroid in zip(clusters, centroids))\n",
    "    \n",
    "    return clusters, centroids, sse, len(clusters), _  \n",
    "\n",
    "def majority_vote(labels):\n",
    "    unique_labels, counts = np.unique(labels, return_counts=True)\n",
    "    majority_label = unique_labels[np.argmax(counts)]\n",
    "    return majority_label\n",
    "\n",
    "def assign_labels_to_clusters(clusters, true_labels):\n",
    "    labels_pred = []\n",
    "\n",
    "    for cluster in clusters:\n",
    "        if len(cluster) > 0:\n",
    "            cluster_labels = true_labels[np.isin(true_labels, cluster)]\n",
    "            if len(cluster_labels) > 0:\n",
    "                majority_label = majority_vote(cluster_labels)\n",
    "                labels_pred.extend([majority_label] * len(cluster))\n",
    "\n",
    "    return np.array(labels_pred)\n",
    "\n",
    "your_data = np.array([[1, 2], [3, 4], [5, 6], [7, 8]])\n",
    "your_true_labels = np.array([0, 1, 1, 0])\n",
    "k = 2 \n",
    "\n",
    "clusters_euclidean, centroids_euclidean, sse_euclidean, num_iterations_euclidean, _ = k_means(your_data, k, euclidean_distance)\n",
    "\n",
    "clusters_cosine, centroids_cosine, sse_cosine, num_iterations_cosine, _ = k_means(your_data, k, cosine_similarity)\n",
    "\n",
    "clusters_jaccard, centroids_jaccard, sse_jaccard, num_iterations_jaccard, _ = k_means(your_data, k, jaccard_similarity)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2) Compare the accuracies of Euclidean-K-means Cosine-K-means, Jarcard-K-means. First, label each cluster using the majority vote label of the data points in that cluster. Later, compute the predictive accuracy of Euclidean-K-means, Cosine-K-means, Jarcard-K-means. Which metric is better?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_pred_euclidean = assign_labels_to_clusters(clusters_euclidean, your_true_labels)\n",
    "labels_pred_cosine = assign_labels_to_clusters(clusters_cosine, your_true_labels)\n",
    "labels_pred_jaccard = assign_labels_to_clusters(clusters_jaccard, your_true_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q3) Set up the same stop criteria: “when there is no change in centroid position OR when the\n",
    "SSE value increases in the next iteration OR when the maximum preset value (e.g., 500, you\n",
    "can set the preset value by yourself) of iteration is complete”, for Euclidean-K-means, Cosine-K-\n",
    "means, Jarcard-K-means. Which method requires more iterations and times to converge? (10\n",
    "points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_means_with_stop_criteria(data, k, distance_function, max_iterations=100, tolerance=1e-4, max_iterations_without_change=10):\n",
    "    centroids = data[np.random.choice(range(len(data)), k, replace=False)]\n",
    "    prev_centroids = centroids.copy()\n",
    "    \n",
    "    for iteration in range(max_iterations):\n",
    "        clusters = [[] for _ in range(k)]\n",
    "        for point in data:\n",
    "            distances = [distance_function(point, centroid) for centroid in centroids]\n",
    "            cluster_assignment = np.argmin(distances)\n",
    "            clusters[cluster_assignment].append(point)\n",
    "        \n",
    "        new_centroids = [np.mean(cluster, axis=0) for cluster in clusters]\n",
    "        \n",
    "        if np.all(np.abs(np.array(new_centroids) - np.array(prev_centroids)) < tolerance):\n",
    "            break\n",
    "        \n",
    "        prev_centroids = new_centroids\n",
    "        centroids = new_centroids\n",
    "    \n",
    "    sse = sum(np.sum((np.array(cluster) - centroid)**2) for cluster, centroid in zip(clusters, centroids))\n",
    "    \n",
    "    return clusters, centroids, sse, iteration + 1 \n",
    "\n",
    "clusters_euclidean, centroids_euclidean, sse_euclidean, num_iterations_euclidean = k_means_with_stop_criteria(your_data, k, euclidean_distance)\n",
    "clusters_cosine, centroids_cosine, sse_cosine, num_iterations_cosine = k_means_with_stop_criteria(your_data, k, cosine_similarity)\n",
    "clusters_jaccard, centroids_jaccard, sse_jaccard, num_iterations_jaccard = k_means_with_stop_criteria(your_data, k, jaccard_similarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q4) Compare the SSEs of Euclidean-K-means Cosine-K-means, Jarcard-K-means with respect to\n",
    "the following three terminating conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_means_with_termination_conditions(data, k, distance_function, max_iterations=100, max_iterations_without_change=10, tolerance=1e-4):\n",
    "    centroids = data[np.random.choice(range(len(data)), k, replace=False)]\n",
    "    prev_centroids = centroids.copy()\n",
    "    \n",
    "    for iteration in range(max_iterations):\n",
    "        clusters = [[] for _ in range(k)]\n",
    "        for point in data:\n",
    "            distances = [distance_function(point, centroid) for centroid in centroids]\n",
    "            cluster_assignment = np.argmin(distances)\n",
    "            clusters[cluster_assignment].append(point)\n",
    "        \n",
    "        new_centroids = [np.mean(cluster, axis=0) for cluster in clusters]\n",
    "        \n",
    "        if (\n",
    "            iteration > 0 and np.all(np.abs(np.array(new_centroids) - np.array(prev_centroids)) < tolerance) or\n",
    "            iteration >= max_iterations_without_change\n",
    "        ):\n",
    "            break\n",
    "        \n",
    "        prev_centroids = new_centroids\n",
    "        centroids = new_centroids\n",
    "    \n",
    "    sse = sum(np.sum((np.array(cluster) - centroid)**2) for cluster, centroid in zip(clusters, centroids))\n",
    "    \n",
    "    return clusters, centroids, sse, iteration + 1 \n",
    "\n",
    "clusters_condition_euclidean, centroids_condition_euclidean, sse_condition_euclidean, num_iterations_condition_euclidean = k_means_with_termination_conditions(your_data, k, euclidean_distance)\n",
    "clusters_condition_cosine, centroids_condition_cosine, sse_condition_cosine, num_iterations_condition_cosine = k_means_with_termination_conditions(your_data, k, cosine_similarity)\n",
    "clusters_condition_jaccard, centroids_condition_jaccard, sse_condition_jaccard, num_iterations_condition_jaccard = k_means_with_termination_conditions(your_data, k, jaccard_similarity)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q5)\n",
    "\n",
    "The exploration of K-means clustering with distinct similarity metrics and termination conditions has unveiled valuable insights. In terms of similarity metrics, Euclidean-K-means excels when clusters exhibit spherical shapes, while Cosine-K-means proves effective in handling high-dimensional and sparse data, considering the angle between data points. Jaccard-K-means emerges as a suitable choice for binary data or datasets emphasizing feature absence.\n",
    "\n",
    "The accuracy comparison, where each cluster is labeled based on majority votes, underscores the considerable influence of the similarity metric on cluster assignments and overall model accuracy. The convergence and iteration analysis reveal that Euclidean-K-means tends to converge faster compared to Cosine-K-means and Jaccard-K-means. The latter may necessitate more iterations, particularly when grappling with high-dimensional or sparse datasets.\n",
    "\n",
    "Examining SSE under different termination conditions provides valuable insights into the behavior of each K-means variant. Conditions such as \"no change in centroid position\" and \"SSE increase\" impact the convergence and stability of clusters, while adjusting the maximum preset value of iterations affects the trade-off between convergence and computational efficiency.\n",
    "\n",
    "Considering algorithmic trade-offs, Euclidean-K-means emerges as computationally efficient and swift to converge, making it suitable for datasets with well-defined clusters. Cosine-K-means, robust in handling high-dimensional sparse data, may require additional iterations for convergence. Jaccard-K-means, effective for binary data, benefits from careful preprocessing to handle sparse datasets efficiently.\n",
    "\n",
    "In real-world applications, the optimal choice between Euclidean-K-means, Cosine-K-means, and Jaccard-K-means hinges on the dataset's characteristics and the specific goals of the clustering task. Striking a balance between accuracy, convergence speed, and computational efficiency is paramount. Thus, experimentation with various configurations and a thoughtful consideration of trade-offs are essential for selecting the most fitting algorithm for a given scenario."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
